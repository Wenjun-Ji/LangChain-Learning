{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建查询分析系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本页面将展示如何在一个基础的端到端示例中使用查询分析。这将涵盖创建一个简单的搜索引擎，展示在将原始用户问题传递给搜索时可能出现的失败模式，然后举例说明查询分析如何帮助解决这个问题。有许多不同的查询分析技术，这个端到端示例不会展示所有这些技术。\n",
    "\n",
    "为了这个示例的目的，我们将对LangChain YouTube视频进行检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "API_SECRET_KEY = \"\"\n",
    "BASE_URL = \"\"  # 代理 base-url 记得加上 /v1\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_SECRET_KEY\n",
    "os.environ[\"OPENAI_API_BASE\"] = BASE_URL\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载文档\n",
    "\n",
    "我们可以用 来加载一些LangChain视频的文字记录：[YouTubeLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.youtube.YoutubeLoader.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
    "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
    "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
    "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
    "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
    "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
    "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
    "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
    "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Add some additional metadata: what year the video was published\n",
    "for doc in docs:\n",
    "    doc.metadata[\"publish_year\"] = int(\n",
    "        datetime.datetime.strptime(\n",
    "            doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
    "        ).strftime(\"%Y\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是我们加载的视频的标题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Auto-Prompt Builder (with Hosted LangServe)',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'SQL Research Assistant',\n",
       " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是与每个视频关联的元数据。我们可以看到，每个文档还有一个标题、浏览次数、发布日期和长度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'HAn9vnJy6S4',\n",
       " 'title': 'OpenGPTs',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 9037,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg',\n",
       " 'publish_date': '2024-01-31 00:00:00',\n",
       " 'length': 1530,\n",
       " 'author': 'LangChain',\n",
       " 'publish_year': 2024}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是文档内容的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be us\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引文档\n",
    "每当我们执行检索时，我们都需要创建一个可以查询的文档索引。我们将使用向量存储来索引我们的文档，我们将首先对它们进行分块，以使我们的检索更加简洁和精确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "EMBEDDING_DEVICE = \"cuda\"\n",
    "embeddings=HuggingFaceEmbeddings(model_name= \"../models/m3e-base\",\n",
    "                                    model_kwargs={\"device\":EMBEDDING_DEVICE}\n",
    "                                    )\n",
    "vectorstore = FAISS.from_documents(\n",
    "    chunked_docs,\n",
    "    embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不带查询分析的检索\n",
    "我们可以直接对用户问题进行相似性搜索，以查找与该问题相关的块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': '28lC4fqukoc', 'title': 'Getting Started with Multi-Modal LLMs', 'description': 'Unknown', 'view_count': 4093, 'thumbnail_url': 'https://i.ytimg.com/vi/28lC4fqukoc/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGCkgWChyMA8=&rs=AOn4CLCPeU4y3IyyG2C3XDHmIYh8efhGbQ', 'publish_date': '2023-12-20 00:00:00', 'length': 1833, 'author': 'LangChain', 'publish_year': 2023}, page_content=\"capacity and conventional rag approaches that just strip the text out really miss a lot of this so let's try kind of how could we build a rag system over the visual content in in a slide deck um so to start off what I did was I took a slide deck and this is um uh data dog's Q3 earnings report I randomly chose it you know it was just like an interesting demonstration of like kind of complex uh you know financial information and figures and slide deck and I created a set of 10 questions and answer pairs about these slides this is like my evalve set um and this is really easy to do I can just create a CSV that has like my question and my answer in this case like my input output pairs um and it's just a set of questions that I devised myself I looked at the slides I said okay here's some interesting question answer pairs I put them in a CSV and I load these into Langs Smith now Langs Smith is Lang chain platform that supports durability and evaluations um and I create a data set for myself in Lang Smith and there's some links down here that show exactly how to do that but that's my starting point so I say okay here's my evaluation set I have the slide deck I built 10 question answer pairs from the slides now let's compare some approaches there might be two different ways to think about multimodal rag um so one is this notion of multimodal embeddings so we take our slides we extract them as images in every image we use multimodal embeddings to map them into this kind of this embedding space that is common between kind of text and and images um for that I use open clip embeddings um and so I now have an index in this case I use chroma that contains a bunch of images uh that have been embedded using open clip um at retrieval time I ask a question I use I basically take the natural language question embed it indeed with multimodal embeddings same ones similarity search just like normal retrieve images that are similar to my question pass the image to in this case uh my\"),\n",
       "  165.43732),\n",
       " (Document(metadata={'source': 'EhlPDL4QrWY', 'title': 'Build and Deploy a RAG app with Pinecone Serverless', 'description': 'Unknown', 'view_count': 9093, 'thumbnail_url': 'https://i.ytimg.com/vi/EhlPDL4QrWY/hq720.jpg', 'publish_date': '2024-01-16 00:00:00', 'length': 1444, 'author': 'LangChain', 'publish_year': 2024}, page_content=\"all the way back up the stack what is rag um setting up a project that uses Lang serve now initially we didn't care about Lang surf we just did prototyping in a notebook so that's great that's easy we defined our chain and look in a lot of cases you only ever wanted a prototype and that's completely fine but what's kind of nice is with Lang serve if you defined your chain using L expression language you can just take take this and just run it as a web app really out of the box and we showed that because we can just Define this chain um and this all we have to do Define our chain we import it add routes here and then the invocation methods of the chain are just HTTP end points in our app and we can call it accordingly using our playground or we can then even go further and host it um as a managed service and it runs on the web uh you can see right here this are hosted app so that's about it I just want to kind of give a quick walk through um and maybe give kudos or folks from uh or friends from uh from Pine Cone for you know the release serverless should be really cool um I've definitely had some pain points with you know pine cone and basically paying for indexes I never use and so I'm really excited for this and uh hopefully this shows you how you can like take an index and very easily kind of build a build an app that can run locally or you can host in the web really easily um and um yeah I'm happy to answer any questions we'll make sure this is all public thanks a lot\"),\n",
       "  169.5216),\n",
       " (Document(metadata={'source': 'HAn9vnJy6S4', 'title': 'OpenGPTs', 'description': 'Unknown', 'view_count': 9037, 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'publish_date': '2024-01-31 00:00:00', 'length': 1530, 'author': 'LangChain', 'publish_year': 2024}, page_content=\"agent types are defined and so different assistants have different architectures that are going on behind the hood let's take a look at the open AI agent for example so important to note this is built on top of L graph so if you aren't familiar with L graph you should definitely go check it out it's a really easy way to build these types of cyclical agentic Frameworks so we have this open AI agent executor which takes in a list of tools an llm a system message and then a checkpoint um and we'll see how we use this so first um we're going to create basically this uh quote unquote agent and this agent is responsible for taking in messages and deciding what to do next so there's first a step where we format um the messages um and so we add a system message um and that's defined up here and then we pass in the rest of the messages the llm also then has access to the tools so we bind it with tools and then we combine it using this pipe syntax to get this agent we next Define a tool executor this is a class that is just does some minor boiler plate for calling tools um and then we start to define the different nodes of the graph so first we Define the function that determines whether to continue or not this is should continue it looks at the messages if there's no tool calls then it finishes if there is tool calls then it will continue and we'll see how we'll use this later on We Now define the node that calls the tools um so here uh we uh take in the list of messages if we we get the last messages we get the last message we know that it involves a function call um because otherwise we would have ended um we get all the tool calls so when there's multiple tool calls we get them all we then pass them in uh into here into the tool executor in a batch so it runs them in parallel um and then we append them to the the messages and we return the messages from this node so importantly this will use a message graph and so this means that every node in the graph should return a\"),\n",
       "  176.54703),\n",
       " (Document(metadata={'source': 'hvAPnpSfSGo', 'title': 'LangGraph: Multi-Agent Workflows', 'description': 'Unknown', 'view_count': 52979, 'thumbnail_url': 'https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg', 'publish_date': '2024-01-23 00:00:00', 'length': 1441, 'author': 'LangChain', 'publish_year': 2024}, page_content=\"to set up everything we've need I've already done this um importantly we're going to be using Lang Smith here to kind of like track the the multi-agents and see what's going on for for a really good debugging experience um Lang Smith right now is in private beta if you don't have access shoot me a DM on LinkedIn or Twitter and we can get you access so the first thing we're going to do do is we're going to Define this helper function that creates an agent so basically we're going to create uh these two kind of like agents up here um and these agents are parameterized by an llm the tools they have and a system message and so then we're taking kind of like uh we're basically creating this mini chain this very simple mini chain that is a prompt which has uh the system message in it um and then it's a it's a call to the llm and it has access to some tool so let's define this helper function we're then going to define the tools that we want the agents to be able to call so we'll have one Search tool we'll use tavil search for that and then we'll have one python tool that can run python code um now we can start to create our graph so first we're going to define the state of what we want to track so we want to track the messages and so again like each agent will add a message to this messages property and then we also want to track who the most recent sender is and the reason reason we need this info is because after we call this tool we're going to check who the sender was and we're going to return to them so this is just some some State that's useful to track over time um so let's define this state um we're now going to Define some agent nodes so here we've created a little helper function to create an agent node and so specifically this agent node is going to uh take in state agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with\"),\n",
       "  179.57474)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"how do I build a RAG agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Started with Multi-Modal LLMs\n",
      "capacity and conventional rag approaches that just strip the text out really miss a lot of this so let's try kind of how could we build a rag system over the visual content in in a slide deck um so to start off what I did was I took a slide deck and this is um uh data dog's Q3 earnings report I randomly chose it you know it was just like an interesting demonstration of like kind of complex uh you know financial information and figures and slide deck and I created a set of 10 questions and answer\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这很有效！我们的第一个结果与这个问题非常相关。\n",
    "\n",
    "如果我们想搜索特定时间段的结果，该怎么办？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Started with Multi-Modal LLMs\n",
      "2023-12-20 00:00:00\n",
      "GPD 4V and some other models that we'll talk about today um so kind of a quick overview of models a lot of this work of course of course you know predates uh you know the current year of 2023 uh it's probably worth noting clip it's very important work from open AI um that kind of map data from different modalities text and images into a shared embedding space um it's open source and actually clip embeddings are still used uh for visual encoding in models that you'll see today for example lava um\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].metadata[\"publish_date\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的第一个结果是 2024 年的（尽管我们要求提供 2023 年的视频），与输入不是很相关。由于我们只是针对文档内容进行搜索，因此无法根据任何文档属性筛选结果。\n",
    "\n",
    "这只是可能出现的一种故障模式。现在让我们来看看查询分析的基本形式是如何解决它的！\n",
    "\n",
    "额，这里我的结果和官方文档不太一样，我的输出就是2023年的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询分析\n",
    "\n",
    "查询分析旨在将用户的自然语言问题转换为结构化的数据库查询，以提高检索结果的准确性和相关性。通过定义查询架构和使用函数调用模型，我们可以将用户的问题转换为包含明确筛选条件的结构化查询。\n",
    "\n",
    "#### 查询架构\n",
    "\n",
    "在这个例子中，查询架构包含一个查询字段和一个可选的发布日期字段。发布日期字段可以包含一个最小值和最大值，用于筛选视频的发布日期。查询架构定义如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Search(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查询生成\n",
    "\n",
    "为了将用户的问题转换为结构化查询，教程使用了 OpenAI 的工具调用 API，特别是新的 `ChatModel.with_structured_output()` 构造函数。这允许将查询架构传递给模型，并让模型输出结构化查询。以下是实现步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0)\n",
    "structured_llm = llm.with_structured_output(Search)\n",
    "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看看我们的分析器为我们之前搜索的问题生成了哪些查询："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query='build RAG agent', publish_year=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"how do I build a RAG agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Search(query='RAG', publish_year=2023)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个地方有的时候跑的出来，有的时候跑不出来，和数学运算的错误类似，都是格式出错\n",
    "query_analyzer.invoke(\"videos on RAG published in 2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用查询分析进行检索\n",
    "\n",
    "我们的查询分析看起来相当不错；现在让我们尝试使用生成的查询实际执行检索。\n",
    "\n",
    "注意：在我们的示例中，我们指定了 `tool_choice=\"Search\"`。这将强制 LLM 调用一个（且仅一个）工具，这意味着我们总是会有一个优化的查询来查找。请注意，这并不总是如此——有关如何处理没有或返回多个优化查询的情况，请参见其他指南。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(search: Search) -> List[Document]:\n",
    "    if search.publish_year is not None:\n",
    "        # This is syntax specific to Chroma,\n",
    "        # the vector database we are using.\n",
    "        _filter = {\"publish_year\": {\"$eq\": search.publish_year}}\n",
    "    else:\n",
    "        _filter = None\n",
    "    return vectorstore.similarity_search(search.query, filter=_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = query_analyzer | retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以在之前有问题的输入上运行这个链，并看到它只产生当年的结果！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Function Search arguments:\n\n{\n  publish_year: 2024,\n  query: \"RAG tutorial\"\n}\n\nare not valid JSON. Received JSONDecodeError Expecting property name enclosed in double quotes: line 2 column 3 (char 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 对于运算格式和json格式的问题 不知道如何处理？\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mretrieval_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRAG tutorial published in 2024\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\Anaconda3\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\runnables\\base.py:2796\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2794\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2795\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2796\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2797\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Program\\Anaconda3\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:183\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m--> 183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    195\u001b[0m             config,\n\u001b[0;32m    196\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    197\u001b[0m         )\n",
      "File \u001b[1;32md:\\Program\\Anaconda3\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\runnables\\base.py:1734\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1730\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1731\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1732\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1733\u001b[0m         Output,\n\u001b[1;32m-> 1734\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1735\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1736\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1737\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m             config,\n\u001b[0;32m   1739\u001b[0m             run_manager,\n\u001b[0;32m   1740\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1741\u001b[0m         ),\n\u001b[0;32m   1742\u001b[0m     )\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1744\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Program\\Anaconda3\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\runnables\\config.py:379\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    378\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Program\\Anaconda3\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:184\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 184\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    188\u001b[0m             config,\n\u001b[0;32m    189\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    190\u001b[0m         )\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    195\u001b[0m             config,\n\u001b[0;32m    196\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    197\u001b[0m         )\n",
      "File \u001b[1;32md:\\Program\\Anaconda3\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\output_parsers\\openai_tools.py:280\u001b[0m, in \u001b[0;36mPydanticToolsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    264\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the result of an LLM call to a list of Pydantic objects.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m        OutputParserException: If the output is not valid JSON.\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m     json_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_results:\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_tool_only \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[1;32md:\\Program\\Anaconda3\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\output_parsers\\openai_tools.py:189\u001b[0m, in \u001b[0;36mJsonOutputToolsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m--> 189\u001b[0m     tool_calls \u001b[38;5;241m=\u001b[39m \u001b[43mparse_tool_calls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tc \u001b[38;5;129;01min\u001b[39;00m tool_calls:\n",
      "File \u001b[1;32md:\\Program\\Anaconda3\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\output_parsers\\openai_tools.py:128\u001b[0m, in \u001b[0;36mparse_tool_calls\u001b[1;34m(raw_tool_calls, partial, strict, return_id)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exceptions:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(exceptions))\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_tools\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Function Search arguments:\n\n{\n  publish_year: 2024,\n  query: \"RAG tutorial\"\n}\n\nare not valid JSON. Received JSONDecodeError Expecting property name enclosed in double quotes: line 2 column 3 (char 4)"
     ]
    }
   ],
   "source": [
    "# 对于运算格式和json格式的问题 不知道如何处理？\n",
    "results = retrieval_chain.invoke(\"RAG tutorial published in 2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(doc.metadata[\"title\"], doc.metadata[\"publish_date\"]) for doc in results]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
